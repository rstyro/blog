---
title: 安装Ollama与使用
tags: [AI]
categories: AI
date: 2025-10-24 19:09:52
---

Ollama 是一个**专为本地运行大型语言模型（LLM）而设计的开源框架**。你可以把它想象成一个“模型管理器”，它让在你自己电脑上部署和使用各种 AI 模型变得非常简单，就像安装和运行一个普通软件一样。

它的核心特点是**轻量级、模块化**，支持同时运行多个不同的模型，并且不需要联网，所有数据处理都在本地完成，非常适合注重隐私、需要离线使用或希望深度定制AI能力的用户。

<!--more-->

选择Ollama进行本地化部署有几个核心优势：

- **数据隐私保障**：敏感数据完全在本地处理，不会外传
- **定制化开发**：可以自由修改模型参数和提示词
- **成本可控**：一次性部署，无需持续支付API调用费用
- **离线可用**：无网络环境仍可运行AI能力
- **多模型支持**：支持同时运行多个不同模型，满足多样化需求



### 1、系统要求与环境准备

#### 硬件要求

| 组件     | 最低要求              | 推荐配置                           |
  | :------- | :-------------------- | :--------------------------------- |
| **CPU**  | 多核处理器（4核以上） | 8核以上，支持 AVX2 指令集          |
| **内存** | 8GB RAM               | 16GB 或更高（大型模型需要 32GB+）  |
| **存储** | 50GB 可用空间         | 200GB+ SSD（模型文件较大）         |
| **GPU**  | 集成显卡（CPU 模式）  | NVIDIA GPU（8GB+ 显存，支持 CUDA） |

#### 软件要求

- **操作系统**：Windows 10/11、macOS 14+、Linux（Ubuntu 18.04+）
- **依赖环境**：.NET Runtime（Windows）、Python 3.8+（可选）


### 2、安装步骤

#### ①、下载安装程序
- 访问 Ollama 官方网站（https://ollama.com/download），选择 "Download for Windows" 按钮下载安装程序（文件名为 `OllamaSetup.exe`）
- **技巧**：国内用户如遇下载速度慢的情况，可尝试使用迅雷等下载工具加速，或将下载链接复制到下载工具中


#### ②、运行安装程序

- 双击下载的 `OllamaSetup.exe`文件，如果出现用户账户控制提示，点击"是"授权安装，**默认是安装在C盘的**。

- **自定义安装路径**：如果希望将 Ollama 安装到非系统盘，可以按照以下步骤操作：

  - 1、打开命令提示符（CMD）或 PowerShell
  - 2、切换到安装程序所在目录
  - 3、执行以下命令（以安装到 D 盘为例）：

```bash
# 打开命令提示符或 PowerShell，执行：
OllamaSetup.exe /DIR="D:\install\ollama"

# 或者使用绝对路径
"C:\Users\用户名\Downloads\OllamaSetup.exe" /DIR="D:\install\ollama"
```

- 推荐使用此方法，可以释放系统盘空间  。

- 安装过程会自动完成，最后关闭窗口即可



#### ③. 验证安装

- 安装完成后，打开命令提示符（CMD）或 PowerShell，输入以下命令验证安装是否成功：

```bash
ollama --version
# 或者
ollama -v
```

- 如果显示版本号（如 `ollama version is 0.12.6`），那么恭喜你，安装成功了


## 二、基本使用教程

安装好 Ollama 后，我们就可以开始“召唤”AI模型了



### 1、下载和运行模型

- Ollama 支持丰富的模型库，包括 Llama、DeepSeek 等热门模型。
- **查看模型库**：访问 Ollama
  - 官方模型库（https://ollama.com/library）选择所需模型 。
  - 检索模型：[https://ollama.com/search](https://ollama.com/search)

- **下载并运行模型**（以 DeepSeek R1 为例）：

```bash
# 下载并运行模型
ollama run deepseek-r1

# 仅下载模型（不运行）
ollama pull deepseek-r1

# 运行已下载的模型
ollama run 模型名称
```


![deepseek模型示例](model.png)

- 其中模型的 **Size（大小）** 指的是模型加载到内存后占用的总存储空间，这包括了**系统内存（RAM）** 和**显存（VRAM）** 的总和。




![ollama运行模型演示图片](ollama2.png)



### 2、Ollama相关命令

- Ollama 提供了多种命令行工具（CLI）供用户与本地运行的模型进行交互。
- 我们可以用 ollama --help 查看包含有哪些命令


```bash
# 启动 Ollama 服务以在后台运行
ollama serve

# 查看当前安装的 Ollama 版本
ollama version

# 查看本地模型列表
ollama list

# 查看模型详细信息
ollama show 模型名称

# 复制模型
ollama cp 原模型名 新模型名

# 删除模型
ollama rm 模型名称

# 停止运行中的模型
ollama stop 模型名称

# 查看运行中的模型
ollama ps
```




## 三、总结

Ollama 极大地降低了在本地使用大型语言模型的门槛，将复杂的部署过程简化为几条简单的命令。



#### 核心优势

- **隐私安全**：数据完全本地处理，保障敏感信息安全
- **成本效益**：一次部署，长期使用，避免持续 API 费用
- **灵活定制**：你可以完全掌控模型行为，进行深度定制和优化，这是云端API无法比拟的
-  **多模型支持**：轻松切换和对比不同模型的优劣，满足多样化需求

#### 适用场景

- **企业内部**：部署在内部服务器上，用于知识库问答、数据分析和代码辅助，无需担心数据泄露。
- **开发测试**：AI应用开发者理想的本地测试环境，方便进行功能验证和提示词工程。
- **离线环境**：对于科研人员、野外工作者或任何需要在无网络环境下使用AI的用户
- **教育研究**：学生和研究人员可以零成本地学习和实验最新的大语言模型技术



#### 技术交流区

1. 你认为本地部署大模型最大的挑战是什么？（性能、成本、易用性？）
2. 除了Ollama，你还用过哪些优秀的本地AI工具？
3. 在工作或学习中，哪个场景下本地AI模型给了你最大的帮助？

**欢迎畅所欲言，每一条有价值的评论我们都会认真阅读！**